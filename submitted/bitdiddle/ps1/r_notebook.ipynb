{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Bitdiddle\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3412e92b8178b5252eac15934731a43d",
     "grade": false,
     "grade_id": "cell-239ecb7ffa6bc86b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "library(testthat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6c7f3544affcdf33558c899b917941fe",
     "grade": false,
     "grade_id": "cell-82082d6a9c660c7e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Calculate the mean of the mpg column of the mtcars data set from scratch (do not use the mean function), save it to a variable called `mean_mpg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ab60d799da385563603c8ec6d9c4bf99",
     "grade": false,
     "grade_id": "cell-9505ddd9bcdffc98",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "mean_mpg = mean(mtcars$hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f95a4031e3b5fb1fe1502e36667a3bd2",
     "grade": true,
     "grade_id": "cell-ab6133cc58e03573",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: Test failed: 'Solution is correct'\n* `mean_mpg` not equal to mean(mtcars$mpg).\n1/1 mismatches\n[1] 147 - 20.1 == 127\n",
     "output_type": "error",
     "traceback": [
      "Error: Test failed: 'Solution is correct'\n* `mean_mpg` not equal to mean(mtcars$mpg).\n1/1 mismatches\n[1] 147 - 20.1 == 127\nTraceback:\n",
      "1. test_that(\"Solution is correct\", {\n .     expect_that(exists(\"mean_mpg\"), is_true())\n .     expect_that(mean_mpg, is_a(\"numeric\"))\n .     expect_equal(mean_mpg, mean(mtcars$mpg))\n . })",
      "2. test_code(desc, code, env = parent.frame())",
      "3. get_reporter()$end_test(context = get_reporter()$.context, test = test)",
      "4. stop(\"Test failed: '\", test, \"'\\n\", messages, call. = FALSE)"
     ]
    }
   ],
   "source": [
    "test_that('Solution is correct', {\n",
    "    expect_that(exists('mean_mpg'), is_true())\n",
    "    expect_that(mean_mpg, is_a(\"numeric\"))\n",
    "    expect_equal(mean_mpg, mean(mtcars$mpg))\n",
    "})\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
